# -*- coding: utf-8 -*-
"""PP0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JG9laWJJd98grnin0APPBNj9zsUKLEPK
"""

import numpy as np
import matplotlib.pyplot as plt
import gym
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import MultivariateNormal

# Step 1: Load Data from Text File
file_path = 'u0002_g_0101v00.txt'

# Load the data manually to handle irregular columns
data = []
with open(file_path, 'r') as file:
    for line in file:
        parts = line.strip().split()
        if len(parts) >= 2:
            try:
                x, y = float(parts[0]), float(parts[1])
                data.append([x, y])
            except ValueError:
                continue

# Convert to a NumPy array
data = np.array(data)

# Extract x and y data
x_data = data[:, 0]
y_data = data[:, 1]

# Step 2: Define Signature Environment
class SignatureEnv(gym.Env):
    def __init__(self, x_data, y_data):
        super(SignatureEnv, self).__init__()
        self.x_data = x_data
        self.y_data = y_data
        self.current_step = 0
        self.trajectory = []

        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)

    def reset(self):
        self.current_step = 0
        self.trajectory = []
        state = np.array([self.x_data[self.current_step], self.y_data[self.current_step]])
        self.trajectory.append(state)
        return state

    def step(self, action):
        self.current_step += 1

        if self.current_step >= len(self.x_data):
            self.current_step = len(self.x_data) - 1

        next_state = np.array([self.x_data[self.current_step], self.y_data[self.current_step]])
        reward = -np.linalg.norm(action - next_state)
        done = self.current_step >= len(self.x_data) - 1

        self.trajectory.append(next_state)

        return next_state, reward, done, {}

    def render(self, mode='human'):
        plt.figure(figsize=(8, 6))
        plt.plot(self.x_data, self.y_data, 'r', label='Original Signature')
        trajectory = np.array(self.trajectory)
        plt.plot(trajectory[:, 0], trajectory[:, 1], 'b', label='Generated Signature')
        plt.title('Signature')
        plt.xlabel('X Coordinate')
        plt.ylabel('Y Coordinate')
        plt.legend()
        plt.grid(True)
        plt.show()

# Step 3: Define the PPO Agent
class PPO(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(PPO, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )

        self.critic = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

        self.action_var = torch.full((action_dim,), 0.5 ** 2)

    def forward(self, state):
        return self.actor(state)

    def act(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        action_mean = self.actor(state)
        cov_matrix = torch.diag(self.action_var).unsqueeze(dim=0)
        dist = MultivariateNormal(action_mean, cov_matrix)
        action = dist.sample()
        action_logprob = dist.log_prob(action)
        return action.detach().numpy()[0], action_logprob.detach()

    def evaluate(self, state, action):
        action_mean = self.actor(state)
        cov_matrix = torch.diag(self.action_var).unsqueeze(dim=0)
        dist = MultivariateNormal(action_mean, cov_matrix)

        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()
        state_value = self.critic(state)

        return action_logprobs, state_value, dist_entropy

# Step 4: Training Loop
def update_parameters(agent, memory, optimizer, gamma, eps_clip, k_epochs):
    # Convert memory to torch tensors
    states = torch.FloatTensor(np.array([m[0] for m in memory]))
    actions = torch.FloatTensor(np.array([m[1] for m in memory]))
    rewards = [m[2] for m in memory]
    old_logprobs = torch.FloatTensor(np.array([m[4] for m in memory]))

    # Compute discounted rewards
    discounted_rewards = []
    R = 0
    for r in reversed(rewards):
        R = r + gamma * R
        discounted_rewards.insert(0, R)
    discounted_rewards = torch.FloatTensor(discounted_rewards)

    # Normalize rewards
    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)

    # Optimize policy for k epochs
    for _ in range(k_epochs):
        logprobs, state_values, dist_entropy = agent.evaluate(states, actions)

        # Calculate ratios
        ratios = torch.exp(logprobs - old_logprobs)

        # Calculate actor loss
        advantages = discounted_rewards.unsqueeze(1) - state_values.detach()
        surr1 = ratios * advantages
        surr2 = torch.clamp(ratios, 1 - eps_clip, 1 + eps_clip) * advantages
        actor_loss = -torch.min(surr1, surr2).mean()

        # Calculate critic loss (fixing the shape mismatch)
        critic_loss = nn.MSELoss()(state_values, discounted_rewards.unsqueeze(1))

        # Total loss
        loss = actor_loss + 0.5 * critic_loss - 0.01 * dist_entropy.mean()

        # Update parameters
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


# Initialize environment and agent
env = SignatureEnv(x_data, y_data)
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]
agent = PPO(state_dim, action_dim)

# Define optimizer and other hyperparameters
optimizer = optim.Adam(agent.parameters(), lr=1e-3)
gamma = 0.99
eps_clip = 0.2
k_epochs = 4
max_steps = 100

# Run training for multiple episodes
num_episodes = 100
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0
    memory = []

    for step in range(max_steps):
        action, action_logprob = agent.act(state)
        next_state, reward, done, _ = env.step(action)

        memory.append((state, action, reward, next_state, action_logprob))

        state = next_state
        episode_reward += reward

        if done:
            break

    update_parameters(agent, memory, optimizer, gamma, eps_clip, k_epochs)

# Render the final trajectory
env.render()