# -*- coding: utf-8 -*-
"""TD3 and PPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QyU4_Z7yW2ZY-Kc-77Ad-AIipBZ-9Xej

TD3 EXAMPLE
"""

import numpy as np
import matplotlib.pyplot as plt
import gym
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
import random

# Step 1: Load Data from Text File
file_path = 'u0003_g_0102v00.txt'

# Load the data manually to handle irregular columns
data = []
with open(file_path, 'r') as file:
    for line in file:
        parts = line.strip().split()
        if len(parts) >= 2:
            try:
                x, y = float(parts[0]), float(parts[1])
                data.append([x, y])
            except ValueError:
                continue

# Convert to a NumPy array
data = np.array(data)

# Extract x and y data
x_data = data[:, 0]
y_data = data[:, 1]

# Normalize data
x_min, x_max = x_data.min(), x_data.max()
y_min, y_max = y_data.min(), y_data.max()

x_data = (x_data - x_min) / (x_max - x_min)
y_data = (y_data - y_min) / (y_max - y_min)

# Step 2: Define Signature Environment
class SignatureEnv(gym.Env):
    def __init__(self, x_data, y_data):
        super(SignatureEnv, self).__init__()
        self.x_data = x_data
        self.y_data = y_data
        self.current_step = 0
        self.trajectory = []  # To store the trajectory

        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)

    def reset(self):
        self.current_step = 0
        self.trajectory = []  # Reset trajectory
        state = np.array([self.x_data[self.current_step], self.y_data[self.current_step]])
        self.trajectory.append(state)
        return state

    def step(self, action):
        self.current_step += 1

        if self.current_step >= len(self.x_data):
            self.current_step = len(self.x_data) - 1

        next_state = np.array([self.x_data[self.current_step], self.y_data[self.current_step]])

        # Updated reward function to penalize deviation more significantly
        reward = -np.square(np.linalg.norm(action - next_state))
        done = self.current_step >= len(self.x_data) - 1

        self.trajectory.append(next_state)

        return next_state, reward, done, {}

    def render(self, mode='human'):
        plt.figure(figsize=(8, 6))
        plt.plot(self.x_data * (x_max - x_min) + x_min, y_data * (y_max - y_min) + y_min, 'r', label='Original Signature')
        trajectory = np.array(self.trajectory)
        plt.plot(trajectory[:, 0] * (x_max - x_min) + x_min, trajectory[:, 1] * (y_max - y_min) + y_min, 'b', label='Generated Signature')
        plt.title('Signature')
        plt.xlabel('X Coordinate')
        plt.ylabel('Y Coordinate')
        plt.legend()
        plt.grid(True)
        plt.show()

# Step 3: Define the TD3 Agent
class TD3(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(TD3, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.critic1 = nn.Sequential(
            nn.Linear(state_dim + action_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        self.critic2 = nn.Sequential(
            nn.Linear(state_dim + action_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

        self.actor_target = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.critic1_target = nn.Sequential(
            nn.Linear(state_dim + action_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        self.critic2_target = nn.Sequential(
            nn.Linear(state_dim + action_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic1_target.load_state_dict(self.critic1.state_dict())
        self.critic2_target.load_state_dict(self.critic2.state_dict())

    def forward(self, x):
        return self.actor(x)

    def get_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        action = self.actor(state)
        return action.detach().numpy()[0]

# Step 4: Training Loop
def soft_update(target, source, tau):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)

def update_parameters(agent, replay_buffer, gamma, tau, policy_noise, noise_clip, policy_delay, batch_size, actor_optimizer, critic1_optimizer, critic2_optimizer, step):
    if len(replay_buffer) < batch_size:
        return

    batch = random.sample(replay_buffer, batch_size)
    state, action, reward, next_state, done = zip(*batch)

    state = torch.FloatTensor(np.array(state))
    action = torch.FloatTensor(np.array(action))
    reward = torch.FloatTensor(np.array(reward)).unsqueeze(1)
    next_state = torch.FloatTensor(np.array(next_state))
    done = torch.FloatTensor(np.array(done)).unsqueeze(1)

    with torch.no_grad():
        noise = (torch.randn_like(action) * policy_noise).clamp(-noise_clip, noise_clip)
        next_action = (agent.actor_target(next_state) + noise).clamp(-1.0, 1.0)

        target_q1 = agent.critic1_target(torch.cat([next_state, next_action], 1))
        target_q2 = agent.critic2_target(torch.cat([next_state, next_action], 1))
        target_q = reward + gamma * (1 - done) * torch.min(target_q1, target_q2)

    current_q1 = agent.critic1(torch.cat([state, action], 1))
    current_q2 = agent.critic2(torch.cat([state, action], 1))

    critic1_loss = nn.MSELoss()(current_q1, target_q)
    critic2_loss = nn.MSELoss()(current_q2, target_q)

    critic1_optimizer.zero_grad()
    critic1_loss.backward()
    critic1_optimizer.step()

    critic2_optimizer.zero_grad()
    critic2_loss.backward()
    critic2_optimizer.step()

    if step % policy_delay == 0:
        actor_loss = -agent.critic1(torch.cat([state, agent.actor(state)], 1)).mean()
        actor_optimizer.zero_grad()
        actor_loss.backward()
        actor_optimizer.step()

        soft_update(agent.critic1_target, agent.critic1, tau)
        soft_update(agent.critic2_target, agent.critic2, tau)
        soft_update(agent.actor_target, agent.actor, tau)

# Initialize environment and agent
env = SignatureEnv(x_data, y_data)
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]
agent = TD3(state_dim, action_dim)

# Define optimizer and other hyperparameters
actor_optimizer = optim.Adam(agent.actor.parameters(), lr=1e-3)
critic1_optimizer = optim.Adam(agent.critic1.parameters(), lr=1e-3)
critic2_optimizer = optim.Adam(agent.critic2.parameters(), lr=1e-3)
gamma = 0.99
tau = 0.005

replay_buffer = []
max_buffer_size = 1000000
batch_size = 64
max_steps = 100
exploration_noise = 0.1
policy_noise = 0.2
noise_clip = 0.5
policy_delay = 2

# Run training for multiple episodes
num_episodes = 100
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0

    for step in range(max_steps):
        action = agent.get_action(state)
        action = action + np.random.normal(0, exploration_noise, size=action_dim)
        action = action.clip(-1.0, 1.0)

        next_state, reward, done, _ = env.step(action)
        replay_buffer.append((state, action, reward, next_state, float(done)))

        if len(replay_buffer) > max_buffer_size:
            replay_buffer.pop(0)

        update_parameters(agent, replay_buffer, gamma, tau, policy_noise, noise_clip, policy_delay, batch_size, actor_optimizer, critic1_optimizer, critic2_optimizer, step)

        state = next_state
        episode_reward += reward

        if done:
            break

# Render the final trajectory
env.render()




